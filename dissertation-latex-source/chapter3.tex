
\chapter{Related Work}

	\section{Term Weighting Modifications}
	TF-IDF is a relatively old approach and there have been many studies comparing the results of TF-IDF with other states of the art term weighting schemes. Also, different researches have suggested novel variants and enhancing algorithms solving various issues. For instance, \cite{RN23} points out the lack of personalization in classic TF-IDF. The authors have highlighted the issue of access to the document corpus for calculating IDF, that is not always available and another issue of ignoring the information from the user’s document collection for recommendations and user modelling. The problem also relates to the concept of frequency distribution in standard TF-IDF. Since the TF and IDF values are calculated based on the occurrence of the same term in other documents as well, so the assumption is that the term is relevant in other documents as well. However, this might not be true, especially in cases of creating user models. Thus, a novel term weighting is suggested, that does not require the document corpus and uses the user’s document collection for user modelling. The term weight formulation in this algorithm is formed by two components, first is TF, which is same as the one in the classic algorithm, that marks the document more relevant based on the high frequency of a term in a document. The calculation for IDF is different from the classic one and is termed as IDuf, that is user-focused and is calculated using the document frequency from the user's collection. And higher relevance is given to the terms that occur less frequently in the document collection.

	In another paper, \cite{RN24} points out the problem of using IDF in text classification. The basic idea behind IDF is that a term occurring frequently has negligible distinguishing power, however, in the case of text classification, this might not be true, because, highly frequent terms in different documents of the same category can be helpful in text classification. Hence, the authors suggested two variants using the supervised learning approach. In one of the approaches, the authors calculate the IDF excluding the category to be considered. In the second approach, the first approach of calculating the IDF is combined with relevancy frequency that considers the category under analysis. However, this model is built out for term weighting in text classification applications and needs to be extended to other fields.
	
    \cite{tian2010improvement} suggested an improvised version of TF-IDF by introducing term distribution. The authors have highlighted the issue of non-reliability on the term frequency since that is based on a single document and cannot be used to as stable differentiating factor between terms and documents.
    To mitigate this issue, the authors have studied different statistical characteristics of terms and suggests the algorithm based on term distribution along with the term frequency. The interesting part of this research is the relation of term distribution with term frequency leads to semantic similarity within documents. Results shown in this paper indicate the significance of term distribution on both the TF and IDF values.
    A similar concept has been used in our research of using term age along with the term weight values to study the similarity of documents with the given topic.


	\cite{RN26} suggests a novel approach to term weighting based on the term positions along with the TF and IDF terms. This research points out the problem of ignoring the term position in the existing retrieval methodologies. Some researches such as  \cite{clarke2000question} have studied the proximity methods for similarity function and suggested that thee documents having small proximity within the query terms are more relevant for the given query. The authors in  \cite{RN26} have extended the same hypothesis and studied the term patterns that occur in the documents using the wavelet transform method. The wavelet transforms breaks the input signal into small waves of different measure and positions. This disintegration of wavelet analyzes the signal at distinct frequency resolution and trace any variation in the position in the signal. This method is usually used in image processing for studying colour variations. A similar approach is used with the stream of text in this case. The low-frequency wavelet represents the scattered terms in a document with a high term frequency and a high-frequency wavelet is used to represent a term that occurs less frequently in the document. With the help of this system, this research shows the wavelet transform method helps in attaining high precision and fetches the results faster with a condensed index. 
	Another contribution of this research is the study of term occurrence patterns. The authors have implemented different types of retrieval methods such to study term patterns, such as Spectral-based document retrieval, and Vector space retrieval. And a comparative analysis of the proximity methods and spectral based retrieval is shown suggesting the low latency in spectral approach. Finally, the research infers that the documents are ranked more relevant if the query terms are close to each other.
	
	In all these researches, we see the authors have tried to fix a flaw in the existing term weighting methodologies. The major idea that we get out them is the existing metrics like term frequency and inverse document frequency are not sufficient to consider different contexts and deal with all different types of use cases that we try to solve from term weighting. This gives us a start in the direction of enhancing the existing term weighting schemes for our research objective.

	\section{Time Aware Models}
	Recommendation system works up to suggest the items to users based on their past choices. For building up such a system, different contexts such as location, time, weather, etc. play a major role, in providing efficient recommendations \cite{adomavicius2011context}. Out of these contexts, time is one of the most useful contexts to track the evolution of user searches and preferences. Utilizing temporal feature has also proved to be an efficient way for recommenders for instance in the Netflix Prize contest and time normalized recommendations are certainly receiving growing application in recent times \cite{RN28}. Major applications for the time-based system in user modelling and recommendation engines have been proposed. The authors have critically analysed the concept of time context in recommendation systems and study the different representations of time. For example, time can be considered as a continuous variable as in the case of when was a particular item searched on a particular site or it can be considered a categorical variable such as season of year, summer, winter, autumn, spring. The best way to exploit all these use cases is to store the exact date and timestamp of the activity that can further be utilized in any kind of recommendation system. Other sources of time factor collection come up from data storing the time of purchase, registration time on a site, etc. These timestamps can be mapped to a particular action or event and then used to form the user model based on the temporal context.
	
	
	Another research \cite{7817104} leverages this concept of using timestamp with event summarization on social media. The authors have proposed a variant of TF-IDF, where previous knowledge of the entire dataset is not needed to get the term weights. The intuition behind this approach is that if a term occurs more frequently over a specific period, they are given higher weighted compared to the ones that occur less frequently. In this research, the authors have highlighted the issue of prior knowledge of entire dataset for calculating standard TF-IDF. This causes the issue in cases when it's streaming data and the weights need to be updated frequently in intervals of specific time frames. For such a scenario, it is suggested to consider the iterative calculation strategy for term weights. Temporal TF-IDF approach considers a set of posts in a cluster that represents a document. And the number of clusters is equal to the number of documents in the corpus. The advantage of this approach is it reduces the computational load on the system and also overrules the need to know the entire corpus at once for calculating Tf-idf. With the cluster of posts, the timeframe is tagged along with it, and that adds not only for the current timeframe but also for the previous timeframe to make the weight distribution more dynamic and relevant to the user. This part of considering two timeframes for computing term weights also helps in relative term weighting and marks the documents more relevant based on the high frequency of occurrence over a specified time frame.
	
	
	One of the researches \cite{RN27}, that falls closest to our research objective, suggests usage of time-normalized term weighting for user modelling. The main aim of this search is to enhance the user experience by building out personalized search results. In the current age scenario, with an expanded usage of the social web interactions, personalized search engines are based on the extraction of user preferences and interests. The intuition in this research is based on the fact that a user's interests change over time, so the personalization of search results should not only be based on the content of the search but also on the time of the search. This bridge of term weight and a user model is filled by using the vector space model, where user preferences are represented as a vector or vectors of keyword and the weight for these keywords are assigned using TF-IDF model or a similar term weighting scheme. It is observed from the weighting schemes that the weights are designated based on the spatial frequency distribution of the keywords without contemplating the diverse factors of them and the user preferences. For instance, suppose a user was living in London for the past many years, and the entire search history is based on London based results and searches. But the user has recently moved to Dublin and has not done much search post moving. In this case, a user model for personalized search would be based on London search history. However, at the moment, a suggestion based on Dublin would be more relevant. In this example, the context of location and time, both play a crucial role in personalization. The research paper \cite{RN27}, considers the elimination of a similar problem by introducing a temporal feature to the term weighting methodology for user modelling. The authors have used the time of social/web search of terms to form the short and long-term contexts and further creating a user profile based on the same. Short term session includes just the current session and the personalization based on the recency and relevancy of recommendations. Long term sessions consider the combination of multiple search sessions.
	
	The main aim of this research is to study the effect of temporal dynamics in user model quality for personalized search framework. And the contribution of this research is the proposal of a personalized search framework, that forms the user model based on the user's web and social media activities. This is done by representing the search interests as weighted term vectors. And the personalized search results are based on both the recency, frequency as well as the persistence of the search keyword.  The comparative study of this algorithm with the standard TF-IDF suggests a significant improvement in results centred on the time normalized user models. Considering this research of temporal context’s effect on term weights, we propose a Time Normalized term weighting algorithms for information retrieval and recommender system, discussed and implemented in this paper.