\chapter{Introduction}

	\section{Background}
	Term Weighting is one of the most crucial tasks in information retrieval and recommender systems. It is a method of quantifying terms in a document to determine the importance of the words in the document and the corpus \cite{RN13}. 
	Apart from recommendation engine and information retrieval, term weighting is effective in many scenarios such as text mining, text classification, duplicate image detection \cite{RN22}, document clustering \cite{RN21}, and even in medical science research \cite{matsuo2018semantic}. In text categorization and data mining, efficient term weighting brings a considerable boost in effectiveness \cite{RN24}. Several term weighting approaches are used in different applications derived from the frequency and distribution of words in documents \cite{RN24}. 
	
	TF-IDF is one of the classic term weighting approaches, that is most frequently used and was found to be used, for instance, by 83\% of text-based research paper recommender systems \cite{RN23}. TF-IDF, as the name suggests, is made up of two parts, term frequency (TF) and inverse document frequency (IDF). TF defines the number of times a term occurs in a document. The basis is that the more frequently a term occurs, the more it is important for the context of the document \cite{RN23}.  IDF is computed as the inverse frequency of documents containing the searched term. The idea behind this is that a rare word should be given higher importance as compared to frequently occurring terms such as articles, pronouns, etc. 
	
	The logic behind most of information retrieval methods is based on similarity functions. These similarity functions take a document and a query as input and generate the score that represents the relevance of the document for the given query \cite{RN26}. These similarity functions are mainly categorized into two categories depending on the method used for quantifying terms. First is the vector space model, where term weight is calculated using the TF-IDF variations, and the second approach is based on probabilistic estimations of terms in the documents \cite{RN14}. There have been numerous researches comparing both the methods and the results mostly depend on the class of problem to be solved, type of queries, size of the corpus, term metrics, etc. Based on these different studies on classic models, numerous extension and alternatives of TF-IDF are suggested. Some other term weighting models used are BM25 \cite{robertson2009probabilistic}, LM Dirichlet, Divergence from independence, etc. 
	
	Some advanced methods, not exactly the term weighting schemes but are used for similarity searches and text classification tasks are based on text and sentence embedding models such as Universal Sentence Encoder (USE) \cite{RN32}, Google’s BERT  \cite{DBLP:journals/corr/abs-1810-04805}, InferSent \cite{DBLP:journals/corr/ConneauKSBB17}, etc. These different approaches depend on the type, size of the corpus, types of queries, and they use different term metrics to determine the effectiveness of term in a document and corpus.
	
	\section{Research Problem}
	In case of information retrieval task, there are certain limitations in standard term weighting approaches. Analyzing the simple approach of TF-IDF, that weights term based on the frequency distribution in the corpus. The real issue in this method is the assumption that frequency distribution remains constant with time, without contemplating the diverse contexts for different terms. In short periods, this holds, however, over longer time this assumption fails. For example, consider two terms, "COVID19" and "neural networks", that have different origin years. And in a specific corpus, that we assume, both these terms occur equal number of times in a document and the entire corpus. Now, in general, there are probably fewer documents containing the term "COVID19" than documents containing the term "neural networks", simply because "COVID19" is a relatively new term, while "neural networks" is a term being used since decades. However, they would be weighted similarly without considering the difference in the origins. The issue that terms have temporal distributions of frequency, not just space distribution is unaccounted when using the standard term weighting methodologies.

    \section{Research Question}
	The research question for this thesis is:
	To study and formulate the temporal distribution of terms and determine its significance in the search relevance by enhancing the standard term weighting methodologies.
	
	\section{Research Objective}
	Considering this uncertainty in term weighting, we suggest a time-normalized term weighting approach, which reflects the age of a term. As the vocabulary changes over time, our intuition is to identify a term’s age based on its first usage and current year and distinguish between the documents based on the age of the terms used. Hence, we propose to weight terms not only on their frequency distributions but also temporal distributions.
	For this research, we have considered three diverse datasets, news, web answer passage, and research paper recommendations, along with the expected set of results formulated by humans as the ground truth. The results are formulated and evaluated based on precision, recall, F1 and NDCG scores. And then, we compare our approach to classic term weighting schemes, that is, TF-IDF, BM25 and USE embedding, to validate the hypothesis.
	Experimental results show substantial improvements over the baseline models for similar recommendations.

\section{Contributions}
The main contributions of this research are:
	\begin{itemize}
	    \item Introduce a concept of term recency parameter or term age and formulation of the same. This is suggested based on the existing metrics in the corpus and/or additional time parameter from outside the corpus.
	    \item Extending the standard state-of-the-art term weighting schemes, TF-IDF, BM25 and USE text embedding to a time normalized version. And hence formulating the newer algorithms as tTF-IDF, tBM25 and tUSE. Similarly, the time normalized version could be extended to other term weighting and embedding models as well.
	\end{itemize}
Furthermore, a research paper for this study has been accepted and presented at the WOSP workshop\footnote{WOSP workshop 2020: https://wosp.core.ac.uk/jcdl2020/index.html} and will be published in the proceedings of the conference. This serves as the contribution to the research community and can be used for further research.